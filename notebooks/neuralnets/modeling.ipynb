{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import keras as k\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, \\\n",
    "                confusion_matrix, classification_report\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1763 entries, 12/27/2013 to 10/24/2018\n",
      "Data columns (total 24 columns):\n",
      "com_count                   1763 non-null int64\n",
      "sub_count                   1763 non-null int64\n",
      "com_body_pos_count          1763 non-null int64\n",
      "com_body_very_pos_count     1763 non-null int64\n",
      "com_body_neg_count          1763 non-null int64\n",
      "com_body_very_neg_count     1763 non-null int64\n",
      "sub_body_pos_count          1763 non-null int64\n",
      "sub_body_very_pos_count     1763 non-null int64\n",
      "sub_body_neg_count          1763 non-null int64\n",
      "sub_body_very_neg_count     1763 non-null int64\n",
      "sub_title_pos_count         1763 non-null int64\n",
      "sub_title_very_pos_count    1763 non-null int64\n",
      "sub_title_neg_count         1763 non-null int64\n",
      "sub_title_very_neg_count    1763 non-null int64\n",
      "avg_clust_coef              1763 non-null float64\n",
      "avg_degree                  1763 non-null float64\n",
      "num_edges                   1763 non-null float64\n",
      "num_nodes                   1763 non-null float64\n",
      "bitcoin_open                1763 non-null float64\n",
      "bitcoin_high                1763 non-null float64\n",
      "bitcoin_low                 1763 non-null float64\n",
      "bitcoin_close               1763 non-null float64\n",
      "bitcoin_volume              1763 non-null int64\n",
      "bitcoin_market_cap          1763 non-null int64\n",
      "dtypes: float64(8), int64(16)\n",
      "memory usage: 344.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('CleanedData.csv',index_col='date')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>com_count</th>\n",
       "      <th>sub_count</th>\n",
       "      <th>com_body_pos_count</th>\n",
       "      <th>com_body_very_pos_count</th>\n",
       "      <th>com_body_neg_count</th>\n",
       "      <th>com_body_very_neg_count</th>\n",
       "      <th>sub_body_pos_count</th>\n",
       "      <th>sub_body_very_pos_count</th>\n",
       "      <th>sub_body_neg_count</th>\n",
       "      <th>sub_body_very_neg_count</th>\n",
       "      <th>sub_title_pos_count</th>\n",
       "      <th>sub_title_very_pos_count</th>\n",
       "      <th>sub_title_neg_count</th>\n",
       "      <th>sub_title_very_neg_count</th>\n",
       "      <th>avg_clust_coef</th>\n",
       "      <th>avg_degree</th>\n",
       "      <th>num_edges</th>\n",
       "      <th>num_nodes</th>\n",
       "      <th>bitcoin_open</th>\n",
       "      <th>bitcoin_high</th>\n",
       "      <th>bitcoin_low</th>\n",
       "      <th>bitcoin_close</th>\n",
       "      <th>bitcoin_volume</th>\n",
       "      <th>bitcoin_market_cap</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12/27/2013</th>\n",
       "      <td>6065</td>\n",
       "      <td>477</td>\n",
       "      <td>1516</td>\n",
       "      <td>766</td>\n",
       "      <td>666</td>\n",
       "      <td>253</td>\n",
       "      <td>105</td>\n",
       "      <td>74</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>67</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006916</td>\n",
       "      <td>1.41</td>\n",
       "      <td>2856.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>763.28</td>\n",
       "      <td>777.51</td>\n",
       "      <td>713.60</td>\n",
       "      <td>735.07</td>\n",
       "      <td>46862700</td>\n",
       "      <td>9295569920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/28/2013</th>\n",
       "      <td>5335</td>\n",
       "      <td>354</td>\n",
       "      <td>1364</td>\n",
       "      <td>659</td>\n",
       "      <td>565</td>\n",
       "      <td>226</td>\n",
       "      <td>85</td>\n",
       "      <td>63</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>54</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>1.43</td>\n",
       "      <td>2689.0</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>737.98</td>\n",
       "      <td>747.06</td>\n",
       "      <td>705.35</td>\n",
       "      <td>727.83</td>\n",
       "      <td>32505800</td>\n",
       "      <td>8990850048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/29/2013</th>\n",
       "      <td>5413</td>\n",
       "      <td>341</td>\n",
       "      <td>1347</td>\n",
       "      <td>649</td>\n",
       "      <td>615</td>\n",
       "      <td>245</td>\n",
       "      <td>92</td>\n",
       "      <td>67</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007134</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2436.0</td>\n",
       "      <td>1877.0</td>\n",
       "      <td>728.05</td>\n",
       "      <td>748.61</td>\n",
       "      <td>714.44</td>\n",
       "      <td>745.05</td>\n",
       "      <td>19011300</td>\n",
       "      <td>8872599552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/30/2013</th>\n",
       "      <td>5757</td>\n",
       "      <td>447</td>\n",
       "      <td>1547</td>\n",
       "      <td>783</td>\n",
       "      <td>659</td>\n",
       "      <td>277</td>\n",
       "      <td>96</td>\n",
       "      <td>65</td>\n",
       "      <td>31</td>\n",
       "      <td>19</td>\n",
       "      <td>48</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004985</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2762.0</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>741.35</td>\n",
       "      <td>766.60</td>\n",
       "      <td>740.24</td>\n",
       "      <td>756.13</td>\n",
       "      <td>20707700</td>\n",
       "      <td>9036999680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/31/2013</th>\n",
       "      <td>4772</td>\n",
       "      <td>352</td>\n",
       "      <td>1162</td>\n",
       "      <td>591</td>\n",
       "      <td>577</td>\n",
       "      <td>224</td>\n",
       "      <td>90</td>\n",
       "      <td>77</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2234.0</td>\n",
       "      <td>1789.0</td>\n",
       "      <td>760.32</td>\n",
       "      <td>760.58</td>\n",
       "      <td>738.17</td>\n",
       "      <td>754.01</td>\n",
       "      <td>20897300</td>\n",
       "      <td>9268240384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            com_count  sub_count  com_body_pos_count  com_body_very_pos_count  \\\n",
       "date                                                                            \n",
       "12/27/2013       6065        477                1516                      766   \n",
       "12/28/2013       5335        354                1364                      659   \n",
       "12/29/2013       5413        341                1347                      649   \n",
       "12/30/2013       5757        447                1547                      783   \n",
       "12/31/2013       4772        352                1162                      591   \n",
       "\n",
       "            com_body_neg_count  com_body_very_neg_count  sub_body_pos_count  \\\n",
       "date                                                                          \n",
       "12/27/2013                 666                      253                 105   \n",
       "12/28/2013                 565                      226                  85   \n",
       "12/29/2013                 615                      245                  92   \n",
       "12/30/2013                 659                      277                  96   \n",
       "12/31/2013                 577                      224                  90   \n",
       "\n",
       "            sub_body_very_pos_count  sub_body_neg_count  \\\n",
       "date                                                      \n",
       "12/27/2013                       74                  14   \n",
       "12/28/2013                       63                  23   \n",
       "12/29/2013                       67                  17   \n",
       "12/30/2013                       65                  31   \n",
       "12/31/2013                       77                  19   \n",
       "\n",
       "            sub_body_very_neg_count  sub_title_pos_count  \\\n",
       "date                                                       \n",
       "12/27/2013                        6                   67   \n",
       "12/28/2013                       14                   54   \n",
       "12/29/2013                        4                   43   \n",
       "12/30/2013                       19                   48   \n",
       "12/31/2013                        7                   50   \n",
       "\n",
       "            sub_title_very_pos_count  sub_title_neg_count  \\\n",
       "date                                                        \n",
       "12/27/2013                        10                   22   \n",
       "12/28/2013                        14                   21   \n",
       "12/29/2013                        10                   25   \n",
       "12/30/2013                        14                   27   \n",
       "12/31/2013                        11                   20   \n",
       "\n",
       "            sub_title_very_neg_count  avg_clust_coef  avg_degree  num_edges  \\\n",
       "date                                                                          \n",
       "12/27/2013                         4        0.006916        1.41     2856.0   \n",
       "12/28/2013                         5        0.004497        1.43     2689.0   \n",
       "12/29/2013                         3        0.007134        1.30     2436.0   \n",
       "12/30/2013                         5        0.004985        1.42     2762.0   \n",
       "12/31/2013                         5        0.003238        1.25     2234.0   \n",
       "\n",
       "            num_nodes  bitcoin_open  bitcoin_high  bitcoin_low  bitcoin_close  \\\n",
       "date                                                                            \n",
       "12/27/2013     2019.0        763.28        777.51       713.60         735.07   \n",
       "12/28/2013     1883.0        737.98        747.06       705.35         727.83   \n",
       "12/29/2013     1877.0        728.05        748.61       714.44         745.05   \n",
       "12/30/2013     1944.0        741.35        766.60       740.24         756.13   \n",
       "12/31/2013     1789.0        760.32        760.58       738.17         754.01   \n",
       "\n",
       "            bitcoin_volume  bitcoin_market_cap  \n",
       "date                                            \n",
       "12/27/2013        46862700          9295569920  \n",
       "12/28/2013        32505800          8990850048  \n",
       "12/29/2013        19011300          8872599552  \n",
       "12/30/2013        20707700          9036999680  \n",
       "12/31/2013        20897300          9268240384  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1328d217978>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show closing price time series\n",
    "df['bitcoin_close'].plot(title='BTC Close Price',grid=True,rot=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1328d570048>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show log-closing price time series\n",
    "df['bitcoin_close'].plot(title='BTC Log-Close Price',grid=True,logy=True,rot=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of days:  1763.0\n",
      "number of months:  58.8\n",
      "number of years:    4.8\n"
     ]
    }
   ],
   "source": [
    "# stats on time series length\n",
    "print(\"number of days:  {:.1f}\".format(df.shape[0]))\n",
    "print(\"number of months:  {:.1f}\".format(df.shape[0] / 30))\n",
    "print(\"number of years:    {:.1f}\".format(df.shape[0] / 365))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to prep data and return DF for modeling\n",
    "\n",
    "def prepare_data(n_lags=1):\n",
    "    # read in  dataset and initialize output df\n",
    "    df = pd.read_csv('CleanedData.csv',index_col='date')\n",
    "    \n",
    "    col_list = df.columns\n",
    "    \n",
    "    \n",
    "    # create lag columns for each other variable\n",
    "    for lag in range(n_lags):\n",
    "        for col in col_list:\n",
    "            df['{}_(-{})'.format(col,lag+1)] = df[col].shift(lag+1)\n",
    "    \n",
    "    # create column to hold tomorrow's close for each day\n",
    "    df['tomorrow_close'] = df['bitcoin_close'].shift(-1)\n",
    "    # change in price between consecutive closing days\n",
    "    df['day_change'] = df['tomorrow_close'] - df['bitcoin_close']\n",
    "    # indicator variable to be used for predicting higher/lower days\n",
    "    df['y'] = np.where(df['day_change'] >= 0, 1, 0)\n",
    "    \n",
    "    # drop intermediate columns\n",
    "    df.drop(['tomorrow_close', 'day_change'], axis=1, inplace=True)\n",
    "    \n",
    "    # return and drop na\n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DF for analysis\n",
    "df = prepare_data(n_lags=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1758 entries, 1/1/2014 to 10/24/2018\n",
      "Columns: 145 entries, com_count to y\n",
      "dtypes: float64(128), int32(1), int64(16)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# inspect df\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>com_count</th>\n",
       "      <th>sub_count</th>\n",
       "      <th>com_body_pos_count</th>\n",
       "      <th>com_body_very_pos_count</th>\n",
       "      <th>com_body_neg_count</th>\n",
       "      <th>com_body_very_neg_count</th>\n",
       "      <th>sub_body_pos_count</th>\n",
       "      <th>sub_body_very_pos_count</th>\n",
       "      <th>sub_body_neg_count</th>\n",
       "      <th>sub_body_very_neg_count</th>\n",
       "      <th>sub_title_pos_count</th>\n",
       "      <th>sub_title_very_pos_count</th>\n",
       "      <th>sub_title_neg_count</th>\n",
       "      <th>sub_title_very_neg_count</th>\n",
       "      <th>avg_clust_coef</th>\n",
       "      <th>avg_degree</th>\n",
       "      <th>num_edges</th>\n",
       "      <th>num_nodes</th>\n",
       "      <th>bitcoin_open</th>\n",
       "      <th>bitcoin_high</th>\n",
       "      <th>bitcoin_low</th>\n",
       "      <th>bitcoin_close</th>\n",
       "      <th>bitcoin_volume</th>\n",
       "      <th>bitcoin_market_cap</th>\n",
       "      <th>com_count_(-1)</th>\n",
       "      <th>sub_count_(-1)</th>\n",
       "      <th>com_body_pos_count_(-1)</th>\n",
       "      <th>com_body_very_pos_count_(-1)</th>\n",
       "      <th>com_body_neg_count_(-1)</th>\n",
       "      <th>com_body_very_neg_count_(-1)</th>\n",
       "      <th>sub_body_pos_count_(-1)</th>\n",
       "      <th>sub_body_very_pos_count_(-1)</th>\n",
       "      <th>sub_body_neg_count_(-1)</th>\n",
       "      <th>sub_body_very_neg_count_(-1)</th>\n",
       "      <th>sub_title_pos_count_(-1)</th>\n",
       "      <th>sub_title_very_pos_count_(-1)</th>\n",
       "      <th>sub_title_neg_count_(-1)</th>\n",
       "      <th>sub_title_very_neg_count_(-1)</th>\n",
       "      <th>avg_clust_coef_(-1)</th>\n",
       "      <th>avg_degree_(-1)</th>\n",
       "      <th>num_edges_(-1)</th>\n",
       "      <th>num_nodes_(-1)</th>\n",
       "      <th>bitcoin_open_(-1)</th>\n",
       "      <th>bitcoin_high_(-1)</th>\n",
       "      <th>bitcoin_low_(-1)</th>\n",
       "      <th>bitcoin_close_(-1)</th>\n",
       "      <th>bitcoin_volume_(-1)</th>\n",
       "      <th>bitcoin_market_cap_(-1)</th>\n",
       "      <th>com_count_(-2)</th>\n",
       "      <th>sub_count_(-2)</th>\n",
       "      <th>com_body_pos_count_(-2)</th>\n",
       "      <th>com_body_very_pos_count_(-2)</th>\n",
       "      <th>com_body_neg_count_(-2)</th>\n",
       "      <th>com_body_very_neg_count_(-2)</th>\n",
       "      <th>sub_body_pos_count_(-2)</th>\n",
       "      <th>sub_body_very_pos_count_(-2)</th>\n",
       "      <th>sub_body_neg_count_(-2)</th>\n",
       "      <th>sub_body_very_neg_count_(-2)</th>\n",
       "      <th>sub_title_pos_count_(-2)</th>\n",
       "      <th>sub_title_very_pos_count_(-2)</th>\n",
       "      <th>sub_title_neg_count_(-2)</th>\n",
       "      <th>sub_title_very_neg_count_(-2)</th>\n",
       "      <th>avg_clust_coef_(-2)</th>\n",
       "      <th>avg_degree_(-2)</th>\n",
       "      <th>num_edges_(-2)</th>\n",
       "      <th>num_nodes_(-2)</th>\n",
       "      <th>bitcoin_open_(-2)</th>\n",
       "      <th>bitcoin_high_(-2)</th>\n",
       "      <th>bitcoin_low_(-2)</th>\n",
       "      <th>bitcoin_close_(-2)</th>\n",
       "      <th>bitcoin_volume_(-2)</th>\n",
       "      <th>bitcoin_market_cap_(-2)</th>\n",
       "      <th>com_count_(-3)</th>\n",
       "      <th>sub_count_(-3)</th>\n",
       "      <th>com_body_pos_count_(-3)</th>\n",
       "      <th>com_body_very_pos_count_(-3)</th>\n",
       "      <th>com_body_neg_count_(-3)</th>\n",
       "      <th>com_body_very_neg_count_(-3)</th>\n",
       "      <th>sub_body_pos_count_(-3)</th>\n",
       "      <th>sub_body_very_pos_count_(-3)</th>\n",
       "      <th>sub_body_neg_count_(-3)</th>\n",
       "      <th>sub_body_very_neg_count_(-3)</th>\n",
       "      <th>sub_title_pos_count_(-3)</th>\n",
       "      <th>sub_title_very_pos_count_(-3)</th>\n",
       "      <th>sub_title_neg_count_(-3)</th>\n",
       "      <th>sub_title_very_neg_count_(-3)</th>\n",
       "      <th>avg_clust_coef_(-3)</th>\n",
       "      <th>avg_degree_(-3)</th>\n",
       "      <th>num_edges_(-3)</th>\n",
       "      <th>num_nodes_(-3)</th>\n",
       "      <th>bitcoin_open_(-3)</th>\n",
       "      <th>bitcoin_high_(-3)</th>\n",
       "      <th>bitcoin_low_(-3)</th>\n",
       "      <th>bitcoin_close_(-3)</th>\n",
       "      <th>bitcoin_volume_(-3)</th>\n",
       "      <th>bitcoin_market_cap_(-3)</th>\n",
       "      <th>com_count_(-4)</th>\n",
       "      <th>sub_count_(-4)</th>\n",
       "      <th>com_body_pos_count_(-4)</th>\n",
       "      <th>com_body_very_pos_count_(-4)</th>\n",
       "      <th>com_body_neg_count_(-4)</th>\n",
       "      <th>com_body_very_neg_count_(-4)</th>\n",
       "      <th>sub_body_pos_count_(-4)</th>\n",
       "      <th>sub_body_very_pos_count_(-4)</th>\n",
       "      <th>sub_body_neg_count_(-4)</th>\n",
       "      <th>sub_body_very_neg_count_(-4)</th>\n",
       "      <th>sub_title_pos_count_(-4)</th>\n",
       "      <th>sub_title_very_pos_count_(-4)</th>\n",
       "      <th>sub_title_neg_count_(-4)</th>\n",
       "      <th>sub_title_very_neg_count_(-4)</th>\n",
       "      <th>avg_clust_coef_(-4)</th>\n",
       "      <th>avg_degree_(-4)</th>\n",
       "      <th>num_edges_(-4)</th>\n",
       "      <th>num_nodes_(-4)</th>\n",
       "      <th>bitcoin_open_(-4)</th>\n",
       "      <th>bitcoin_high_(-4)</th>\n",
       "      <th>bitcoin_low_(-4)</th>\n",
       "      <th>bitcoin_close_(-4)</th>\n",
       "      <th>bitcoin_volume_(-4)</th>\n",
       "      <th>bitcoin_market_cap_(-4)</th>\n",
       "      <th>com_count_(-5)</th>\n",
       "      <th>sub_count_(-5)</th>\n",
       "      <th>com_body_pos_count_(-5)</th>\n",
       "      <th>com_body_very_pos_count_(-5)</th>\n",
       "      <th>com_body_neg_count_(-5)</th>\n",
       "      <th>com_body_very_neg_count_(-5)</th>\n",
       "      <th>sub_body_pos_count_(-5)</th>\n",
       "      <th>sub_body_very_pos_count_(-5)</th>\n",
       "      <th>sub_body_neg_count_(-5)</th>\n",
       "      <th>sub_body_very_neg_count_(-5)</th>\n",
       "      <th>sub_title_pos_count_(-5)</th>\n",
       "      <th>sub_title_very_pos_count_(-5)</th>\n",
       "      <th>sub_title_neg_count_(-5)</th>\n",
       "      <th>sub_title_very_neg_count_(-5)</th>\n",
       "      <th>avg_clust_coef_(-5)</th>\n",
       "      <th>avg_degree_(-5)</th>\n",
       "      <th>num_edges_(-5)</th>\n",
       "      <th>num_nodes_(-5)</th>\n",
       "      <th>bitcoin_open_(-5)</th>\n",
       "      <th>bitcoin_high_(-5)</th>\n",
       "      <th>bitcoin_low_(-5)</th>\n",
       "      <th>bitcoin_close_(-5)</th>\n",
       "      <th>bitcoin_volume_(-5)</th>\n",
       "      <th>bitcoin_market_cap_(-5)</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1/1/2014</th>\n",
       "      <td>4173</td>\n",
       "      <td>288</td>\n",
       "      <td>997</td>\n",
       "      <td>496</td>\n",
       "      <td>468</td>\n",
       "      <td>172</td>\n",
       "      <td>63</td>\n",
       "      <td>47</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0.006140</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>1581.0</td>\n",
       "      <td>754.97</td>\n",
       "      <td>775.35</td>\n",
       "      <td>754.97</td>\n",
       "      <td>771.40</td>\n",
       "      <td>22489400</td>\n",
       "      <td>9203030016</td>\n",
       "      <td>4772.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>591.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2234.0</td>\n",
       "      <td>1789.0</td>\n",
       "      <td>760.32</td>\n",
       "      <td>760.58</td>\n",
       "      <td>738.17</td>\n",
       "      <td>754.01</td>\n",
       "      <td>20897300.0</td>\n",
       "      <td>9.268240e+09</td>\n",
       "      <td>5757.0</td>\n",
       "      <td>447.0</td>\n",
       "      <td>1547.0</td>\n",
       "      <td>783.0</td>\n",
       "      <td>659.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.004985</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2762.0</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>741.35</td>\n",
       "      <td>766.60</td>\n",
       "      <td>740.24</td>\n",
       "      <td>756.13</td>\n",
       "      <td>20707700.0</td>\n",
       "      <td>9.037000e+09</td>\n",
       "      <td>5413.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>649.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.007134</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2436.0</td>\n",
       "      <td>1877.0</td>\n",
       "      <td>728.05</td>\n",
       "      <td>748.61</td>\n",
       "      <td>714.44</td>\n",
       "      <td>745.05</td>\n",
       "      <td>19011300.0</td>\n",
       "      <td>8.872600e+09</td>\n",
       "      <td>5335.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>1364.0</td>\n",
       "      <td>659.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>1.43</td>\n",
       "      <td>2689.0</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>737.98</td>\n",
       "      <td>747.06</td>\n",
       "      <td>705.35</td>\n",
       "      <td>727.83</td>\n",
       "      <td>32505800.0</td>\n",
       "      <td>8.990850e+09</td>\n",
       "      <td>6065.0</td>\n",
       "      <td>477.0</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>766.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.006916</td>\n",
       "      <td>1.41</td>\n",
       "      <td>2856.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>763.28</td>\n",
       "      <td>777.51</td>\n",
       "      <td>713.60</td>\n",
       "      <td>735.07</td>\n",
       "      <td>46862700.0</td>\n",
       "      <td>9.295570e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/2/2014</th>\n",
       "      <td>5481</td>\n",
       "      <td>483</td>\n",
       "      <td>1351</td>\n",
       "      <td>619</td>\n",
       "      <td>533</td>\n",
       "      <td>212</td>\n",
       "      <td>96</td>\n",
       "      <td>67</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>73</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005718</td>\n",
       "      <td>1.28</td>\n",
       "      <td>2458.0</td>\n",
       "      <td>1913.0</td>\n",
       "      <td>773.44</td>\n",
       "      <td>820.31</td>\n",
       "      <td>767.21</td>\n",
       "      <td>802.39</td>\n",
       "      <td>38489500</td>\n",
       "      <td>9428179968</td>\n",
       "      <td>4173.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>997.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.006140</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>1581.0</td>\n",
       "      <td>754.97</td>\n",
       "      <td>775.35</td>\n",
       "      <td>754.97</td>\n",
       "      <td>771.40</td>\n",
       "      <td>22489400.0</td>\n",
       "      <td>9.203030e+09</td>\n",
       "      <td>4772.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>591.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2234.0</td>\n",
       "      <td>1789.0</td>\n",
       "      <td>760.32</td>\n",
       "      <td>760.58</td>\n",
       "      <td>738.17</td>\n",
       "      <td>754.01</td>\n",
       "      <td>20897300.0</td>\n",
       "      <td>9.268240e+09</td>\n",
       "      <td>5757.0</td>\n",
       "      <td>447.0</td>\n",
       "      <td>1547.0</td>\n",
       "      <td>783.0</td>\n",
       "      <td>659.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.004985</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2762.0</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>741.35</td>\n",
       "      <td>766.60</td>\n",
       "      <td>740.24</td>\n",
       "      <td>756.13</td>\n",
       "      <td>20707700.0</td>\n",
       "      <td>9.037000e+09</td>\n",
       "      <td>5413.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>649.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.007134</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2436.0</td>\n",
       "      <td>1877.0</td>\n",
       "      <td>728.05</td>\n",
       "      <td>748.61</td>\n",
       "      <td>714.44</td>\n",
       "      <td>745.05</td>\n",
       "      <td>19011300.0</td>\n",
       "      <td>8.872600e+09</td>\n",
       "      <td>5335.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>1364.0</td>\n",
       "      <td>659.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>1.43</td>\n",
       "      <td>2689.0</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>737.98</td>\n",
       "      <td>747.06</td>\n",
       "      <td>705.35</td>\n",
       "      <td>727.83</td>\n",
       "      <td>32505800.0</td>\n",
       "      <td>8.990850e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/3/2014</th>\n",
       "      <td>6209</td>\n",
       "      <td>531</td>\n",
       "      <td>1659</td>\n",
       "      <td>837</td>\n",
       "      <td>633</td>\n",
       "      <td>270</td>\n",
       "      <td>106</td>\n",
       "      <td>74</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>86</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004442</td>\n",
       "      <td>1.32</td>\n",
       "      <td>2701.0</td>\n",
       "      <td>2046.0</td>\n",
       "      <td>802.85</td>\n",
       "      <td>834.15</td>\n",
       "      <td>789.12</td>\n",
       "      <td>818.72</td>\n",
       "      <td>37810100</td>\n",
       "      <td>9786680320</td>\n",
       "      <td>5481.0</td>\n",
       "      <td>483.0</td>\n",
       "      <td>1351.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005718</td>\n",
       "      <td>1.28</td>\n",
       "      <td>2458.0</td>\n",
       "      <td>1913.0</td>\n",
       "      <td>773.44</td>\n",
       "      <td>820.31</td>\n",
       "      <td>767.21</td>\n",
       "      <td>802.39</td>\n",
       "      <td>38489500.0</td>\n",
       "      <td>9.428180e+09</td>\n",
       "      <td>4173.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>997.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.006140</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>1581.0</td>\n",
       "      <td>754.97</td>\n",
       "      <td>775.35</td>\n",
       "      <td>754.97</td>\n",
       "      <td>771.40</td>\n",
       "      <td>22489400.0</td>\n",
       "      <td>9.203030e+09</td>\n",
       "      <td>4772.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>591.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2234.0</td>\n",
       "      <td>1789.0</td>\n",
       "      <td>760.32</td>\n",
       "      <td>760.58</td>\n",
       "      <td>738.17</td>\n",
       "      <td>754.01</td>\n",
       "      <td>20897300.0</td>\n",
       "      <td>9.268240e+09</td>\n",
       "      <td>5757.0</td>\n",
       "      <td>447.0</td>\n",
       "      <td>1547.0</td>\n",
       "      <td>783.0</td>\n",
       "      <td>659.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.004985</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2762.0</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>741.35</td>\n",
       "      <td>766.60</td>\n",
       "      <td>740.24</td>\n",
       "      <td>756.13</td>\n",
       "      <td>20707700.0</td>\n",
       "      <td>9.037000e+09</td>\n",
       "      <td>5413.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>649.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.007134</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2436.0</td>\n",
       "      <td>1877.0</td>\n",
       "      <td>728.05</td>\n",
       "      <td>748.61</td>\n",
       "      <td>714.44</td>\n",
       "      <td>745.05</td>\n",
       "      <td>19011300.0</td>\n",
       "      <td>8.872600e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/4/2014</th>\n",
       "      <td>6381</td>\n",
       "      <td>421</td>\n",
       "      <td>1598</td>\n",
       "      <td>826</td>\n",
       "      <td>699</td>\n",
       "      <td>295</td>\n",
       "      <td>115</td>\n",
       "      <td>81</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>65</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.014446</td>\n",
       "      <td>1.44</td>\n",
       "      <td>2981.0</td>\n",
       "      <td>2064.0</td>\n",
       "      <td>823.27</td>\n",
       "      <td>859.51</td>\n",
       "      <td>801.67</td>\n",
       "      <td>859.51</td>\n",
       "      <td>38005000</td>\n",
       "      <td>10035600384</td>\n",
       "      <td>6209.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>1659.0</td>\n",
       "      <td>837.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004442</td>\n",
       "      <td>1.32</td>\n",
       "      <td>2701.0</td>\n",
       "      <td>2046.0</td>\n",
       "      <td>802.85</td>\n",
       "      <td>834.15</td>\n",
       "      <td>789.12</td>\n",
       "      <td>818.72</td>\n",
       "      <td>37810100.0</td>\n",
       "      <td>9.786680e+09</td>\n",
       "      <td>5481.0</td>\n",
       "      <td>483.0</td>\n",
       "      <td>1351.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005718</td>\n",
       "      <td>1.28</td>\n",
       "      <td>2458.0</td>\n",
       "      <td>1913.0</td>\n",
       "      <td>773.44</td>\n",
       "      <td>820.31</td>\n",
       "      <td>767.21</td>\n",
       "      <td>802.39</td>\n",
       "      <td>38489500.0</td>\n",
       "      <td>9.428180e+09</td>\n",
       "      <td>4173.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>997.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.006140</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>1581.0</td>\n",
       "      <td>754.97</td>\n",
       "      <td>775.35</td>\n",
       "      <td>754.97</td>\n",
       "      <td>771.40</td>\n",
       "      <td>22489400.0</td>\n",
       "      <td>9.203030e+09</td>\n",
       "      <td>4772.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>591.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2234.0</td>\n",
       "      <td>1789.0</td>\n",
       "      <td>760.32</td>\n",
       "      <td>760.58</td>\n",
       "      <td>738.17</td>\n",
       "      <td>754.01</td>\n",
       "      <td>20897300.0</td>\n",
       "      <td>9.268240e+09</td>\n",
       "      <td>5757.0</td>\n",
       "      <td>447.0</td>\n",
       "      <td>1547.0</td>\n",
       "      <td>783.0</td>\n",
       "      <td>659.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.004985</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2762.0</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>741.35</td>\n",
       "      <td>766.60</td>\n",
       "      <td>740.24</td>\n",
       "      <td>756.13</td>\n",
       "      <td>20707700.0</td>\n",
       "      <td>9.037000e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/5/2014</th>\n",
       "      <td>6492</td>\n",
       "      <td>469</td>\n",
       "      <td>1549</td>\n",
       "      <td>721</td>\n",
       "      <td>630</td>\n",
       "      <td>236</td>\n",
       "      <td>131</td>\n",
       "      <td>86</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>61</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005545</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3192.0</td>\n",
       "      <td>2149.0</td>\n",
       "      <td>858.55</td>\n",
       "      <td>952.40</td>\n",
       "      <td>854.52</td>\n",
       "      <td>933.53</td>\n",
       "      <td>72898496</td>\n",
       "      <td>10465699840</td>\n",
       "      <td>6381.0</td>\n",
       "      <td>421.0</td>\n",
       "      <td>1598.0</td>\n",
       "      <td>826.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.014446</td>\n",
       "      <td>1.44</td>\n",
       "      <td>2981.0</td>\n",
       "      <td>2064.0</td>\n",
       "      <td>823.27</td>\n",
       "      <td>859.51</td>\n",
       "      <td>801.67</td>\n",
       "      <td>859.51</td>\n",
       "      <td>38005000.0</td>\n",
       "      <td>1.003560e+10</td>\n",
       "      <td>6209.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>1659.0</td>\n",
       "      <td>837.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004442</td>\n",
       "      <td>1.32</td>\n",
       "      <td>2701.0</td>\n",
       "      <td>2046.0</td>\n",
       "      <td>802.85</td>\n",
       "      <td>834.15</td>\n",
       "      <td>789.12</td>\n",
       "      <td>818.72</td>\n",
       "      <td>37810100.0</td>\n",
       "      <td>9.786680e+09</td>\n",
       "      <td>5481.0</td>\n",
       "      <td>483.0</td>\n",
       "      <td>1351.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005718</td>\n",
       "      <td>1.28</td>\n",
       "      <td>2458.0</td>\n",
       "      <td>1913.0</td>\n",
       "      <td>773.44</td>\n",
       "      <td>820.31</td>\n",
       "      <td>767.21</td>\n",
       "      <td>802.39</td>\n",
       "      <td>38489500.0</td>\n",
       "      <td>9.428180e+09</td>\n",
       "      <td>4173.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>997.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.006140</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>1581.0</td>\n",
       "      <td>754.97</td>\n",
       "      <td>775.35</td>\n",
       "      <td>754.97</td>\n",
       "      <td>771.40</td>\n",
       "      <td>22489400.0</td>\n",
       "      <td>9.203030e+09</td>\n",
       "      <td>4772.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>591.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2234.0</td>\n",
       "      <td>1789.0</td>\n",
       "      <td>760.32</td>\n",
       "      <td>760.58</td>\n",
       "      <td>738.17</td>\n",
       "      <td>754.01</td>\n",
       "      <td>20897300.0</td>\n",
       "      <td>9.268240e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          com_count  sub_count  com_body_pos_count  com_body_very_pos_count  \\\n",
       "date                                                                          \n",
       "1/1/2014       4173        288                 997                      496   \n",
       "1/2/2014       5481        483                1351                      619   \n",
       "1/3/2014       6209        531                1659                      837   \n",
       "1/4/2014       6381        421                1598                      826   \n",
       "1/5/2014       6492        469                1549                      721   \n",
       "\n",
       "          com_body_neg_count  com_body_very_neg_count  sub_body_pos_count  \\\n",
       "date                                                                        \n",
       "1/1/2014                 468                      172                  63   \n",
       "1/2/2014                 533                      212                  96   \n",
       "1/3/2014                 633                      270                 106   \n",
       "1/4/2014                 699                      295                 115   \n",
       "1/5/2014                 630                      236                 131   \n",
       "\n",
       "          sub_body_very_pos_count  sub_body_neg_count  \\\n",
       "date                                                    \n",
       "1/1/2014                       47                  17   \n",
       "1/2/2014                       67                  18   \n",
       "1/3/2014                       74                  22   \n",
       "1/4/2014                       81                  19   \n",
       "1/5/2014                       86                  22   \n",
       "\n",
       "          sub_body_very_neg_count  sub_title_pos_count  \\\n",
       "date                                                     \n",
       "1/1/2014                        9                   42   \n",
       "1/2/2014                       12                   73   \n",
       "1/3/2014                        8                   86   \n",
       "1/4/2014                        8                   65   \n",
       "1/5/2014                        9                   61   \n",
       "\n",
       "          sub_title_very_pos_count  sub_title_neg_count  \\\n",
       "date                                                      \n",
       "1/1/2014                         8                   13   \n",
       "1/2/2014                        12                   21   \n",
       "1/3/2014                        10                   19   \n",
       "1/4/2014                        16                   18   \n",
       "1/5/2014                        13                   21   \n",
       "\n",
       "          sub_title_very_neg_count  avg_clust_coef  avg_degree  num_edges  \\\n",
       "date                                                                        \n",
       "1/1/2014                         3        0.006140        1.20     1900.0   \n",
       "1/2/2014                         2        0.005718        1.28     2458.0   \n",
       "1/3/2014                         0        0.004442        1.32     2701.0   \n",
       "1/4/2014                         2        0.014446        1.44     2981.0   \n",
       "1/5/2014                         5        0.005545        1.49     3192.0   \n",
       "\n",
       "          num_nodes  bitcoin_open  bitcoin_high  bitcoin_low  bitcoin_close  \\\n",
       "date                                                                          \n",
       "1/1/2014     1581.0        754.97        775.35       754.97         771.40   \n",
       "1/2/2014     1913.0        773.44        820.31       767.21         802.39   \n",
       "1/3/2014     2046.0        802.85        834.15       789.12         818.72   \n",
       "1/4/2014     2064.0        823.27        859.51       801.67         859.51   \n",
       "1/5/2014     2149.0        858.55        952.40       854.52         933.53   \n",
       "\n",
       "          bitcoin_volume  bitcoin_market_cap  com_count_(-1)  sub_count_(-1)  \\\n",
       "date                                                                           \n",
       "1/1/2014        22489400          9203030016          4772.0           352.0   \n",
       "1/2/2014        38489500          9428179968          4173.0           288.0   \n",
       "1/3/2014        37810100          9786680320          5481.0           483.0   \n",
       "1/4/2014        38005000         10035600384          6209.0           531.0   \n",
       "1/5/2014        72898496         10465699840          6381.0           421.0   \n",
       "\n",
       "          com_body_pos_count_(-1)  com_body_very_pos_count_(-1)  \\\n",
       "date                                                              \n",
       "1/1/2014                   1162.0                         591.0   \n",
       "1/2/2014                    997.0                         496.0   \n",
       "1/3/2014                   1351.0                         619.0   \n",
       "1/4/2014                   1659.0                         837.0   \n",
       "1/5/2014                   1598.0                         826.0   \n",
       "\n",
       "          com_body_neg_count_(-1)  com_body_very_neg_count_(-1)  \\\n",
       "date                                                              \n",
       "1/1/2014                    577.0                         224.0   \n",
       "1/2/2014                    468.0                         172.0   \n",
       "1/3/2014                    533.0                         212.0   \n",
       "1/4/2014                    633.0                         270.0   \n",
       "1/5/2014                    699.0                         295.0   \n",
       "\n",
       "          sub_body_pos_count_(-1)  sub_body_very_pos_count_(-1)  \\\n",
       "date                                                              \n",
       "1/1/2014                     90.0                          77.0   \n",
       "1/2/2014                     63.0                          47.0   \n",
       "1/3/2014                     96.0                          67.0   \n",
       "1/4/2014                    106.0                          74.0   \n",
       "1/5/2014                    115.0                          81.0   \n",
       "\n",
       "          sub_body_neg_count_(-1)  sub_body_very_neg_count_(-1)  \\\n",
       "date                                                              \n",
       "1/1/2014                     19.0                           7.0   \n",
       "1/2/2014                     17.0                           9.0   \n",
       "1/3/2014                     18.0                          12.0   \n",
       "1/4/2014                     22.0                           8.0   \n",
       "1/5/2014                     19.0                           8.0   \n",
       "\n",
       "          sub_title_pos_count_(-1)  sub_title_very_pos_count_(-1)  \\\n",
       "date                                                                \n",
       "1/1/2014                      50.0                           11.0   \n",
       "1/2/2014                      42.0                            8.0   \n",
       "1/3/2014                      73.0                           12.0   \n",
       "1/4/2014                      86.0                           10.0   \n",
       "1/5/2014                      65.0                           16.0   \n",
       "\n",
       "          sub_title_neg_count_(-1)  sub_title_very_neg_count_(-1)  \\\n",
       "date                                                                \n",
       "1/1/2014                      20.0                            5.0   \n",
       "1/2/2014                      13.0                            3.0   \n",
       "1/3/2014                      21.0                            2.0   \n",
       "1/4/2014                      19.0                            0.0   \n",
       "1/5/2014                      18.0                            2.0   \n",
       "\n",
       "          avg_clust_coef_(-1)  avg_degree_(-1)  num_edges_(-1)  \\\n",
       "date                                                             \n",
       "1/1/2014             0.003238             1.25          2234.0   \n",
       "1/2/2014             0.006140             1.20          1900.0   \n",
       "1/3/2014             0.005718             1.28          2458.0   \n",
       "1/4/2014             0.004442             1.32          2701.0   \n",
       "1/5/2014             0.014446             1.44          2981.0   \n",
       "\n",
       "          num_nodes_(-1)  bitcoin_open_(-1)  bitcoin_high_(-1)  \\\n",
       "date                                                             \n",
       "1/1/2014          1789.0             760.32             760.58   \n",
       "1/2/2014          1581.0             754.97             775.35   \n",
       "1/3/2014          1913.0             773.44             820.31   \n",
       "1/4/2014          2046.0             802.85             834.15   \n",
       "1/5/2014          2064.0             823.27             859.51   \n",
       "\n",
       "          bitcoin_low_(-1)  bitcoin_close_(-1)  bitcoin_volume_(-1)  \\\n",
       "date                                                                  \n",
       "1/1/2014            738.17              754.01           20897300.0   \n",
       "1/2/2014            754.97              771.40           22489400.0   \n",
       "1/3/2014            767.21              802.39           38489500.0   \n",
       "1/4/2014            789.12              818.72           37810100.0   \n",
       "1/5/2014            801.67              859.51           38005000.0   \n",
       "\n",
       "          bitcoin_market_cap_(-1)  com_count_(-2)  sub_count_(-2)  \\\n",
       "date                                                                \n",
       "1/1/2014             9.268240e+09          5757.0           447.0   \n",
       "1/2/2014             9.203030e+09          4772.0           352.0   \n",
       "1/3/2014             9.428180e+09          4173.0           288.0   \n",
       "1/4/2014             9.786680e+09          5481.0           483.0   \n",
       "1/5/2014             1.003560e+10          6209.0           531.0   \n",
       "\n",
       "          com_body_pos_count_(-2)  com_body_very_pos_count_(-2)  \\\n",
       "date                                                              \n",
       "1/1/2014                   1547.0                         783.0   \n",
       "1/2/2014                   1162.0                         591.0   \n",
       "1/3/2014                    997.0                         496.0   \n",
       "1/4/2014                   1351.0                         619.0   \n",
       "1/5/2014                   1659.0                         837.0   \n",
       "\n",
       "          com_body_neg_count_(-2)  com_body_very_neg_count_(-2)  \\\n",
       "date                                                              \n",
       "1/1/2014                    659.0                         277.0   \n",
       "1/2/2014                    577.0                         224.0   \n",
       "1/3/2014                    468.0                         172.0   \n",
       "1/4/2014                    533.0                         212.0   \n",
       "1/5/2014                    633.0                         270.0   \n",
       "\n",
       "          sub_body_pos_count_(-2)  sub_body_very_pos_count_(-2)  \\\n",
       "date                                                              \n",
       "1/1/2014                     96.0                          65.0   \n",
       "1/2/2014                     90.0                          77.0   \n",
       "1/3/2014                     63.0                          47.0   \n",
       "1/4/2014                     96.0                          67.0   \n",
       "1/5/2014                    106.0                          74.0   \n",
       "\n",
       "          sub_body_neg_count_(-2)  sub_body_very_neg_count_(-2)  \\\n",
       "date                                                              \n",
       "1/1/2014                     31.0                          19.0   \n",
       "1/2/2014                     19.0                           7.0   \n",
       "1/3/2014                     17.0                           9.0   \n",
       "1/4/2014                     18.0                          12.0   \n",
       "1/5/2014                     22.0                           8.0   \n",
       "\n",
       "          sub_title_pos_count_(-2)  sub_title_very_pos_count_(-2)  \\\n",
       "date                                                                \n",
       "1/1/2014                      48.0                           14.0   \n",
       "1/2/2014                      50.0                           11.0   \n",
       "1/3/2014                      42.0                            8.0   \n",
       "1/4/2014                      73.0                           12.0   \n",
       "1/5/2014                      86.0                           10.0   \n",
       "\n",
       "          sub_title_neg_count_(-2)  sub_title_very_neg_count_(-2)  \\\n",
       "date                                                                \n",
       "1/1/2014                      27.0                            5.0   \n",
       "1/2/2014                      20.0                            5.0   \n",
       "1/3/2014                      13.0                            3.0   \n",
       "1/4/2014                      21.0                            2.0   \n",
       "1/5/2014                      19.0                            0.0   \n",
       "\n",
       "          avg_clust_coef_(-2)  avg_degree_(-2)  num_edges_(-2)  \\\n",
       "date                                                             \n",
       "1/1/2014             0.004985             1.42          2762.0   \n",
       "1/2/2014             0.003238             1.25          2234.0   \n",
       "1/3/2014             0.006140             1.20          1900.0   \n",
       "1/4/2014             0.005718             1.28          2458.0   \n",
       "1/5/2014             0.004442             1.32          2701.0   \n",
       "\n",
       "          num_nodes_(-2)  bitcoin_open_(-2)  bitcoin_high_(-2)  \\\n",
       "date                                                             \n",
       "1/1/2014          1944.0             741.35             766.60   \n",
       "1/2/2014          1789.0             760.32             760.58   \n",
       "1/3/2014          1581.0             754.97             775.35   \n",
       "1/4/2014          1913.0             773.44             820.31   \n",
       "1/5/2014          2046.0             802.85             834.15   \n",
       "\n",
       "          bitcoin_low_(-2)  bitcoin_close_(-2)  bitcoin_volume_(-2)  \\\n",
       "date                                                                  \n",
       "1/1/2014            740.24              756.13           20707700.0   \n",
       "1/2/2014            738.17              754.01           20897300.0   \n",
       "1/3/2014            754.97              771.40           22489400.0   \n",
       "1/4/2014            767.21              802.39           38489500.0   \n",
       "1/5/2014            789.12              818.72           37810100.0   \n",
       "\n",
       "          bitcoin_market_cap_(-2)  com_count_(-3)  sub_count_(-3)  \\\n",
       "date                                                                \n",
       "1/1/2014             9.037000e+09          5413.0           341.0   \n",
       "1/2/2014             9.268240e+09          5757.0           447.0   \n",
       "1/3/2014             9.203030e+09          4772.0           352.0   \n",
       "1/4/2014             9.428180e+09          4173.0           288.0   \n",
       "1/5/2014             9.786680e+09          5481.0           483.0   \n",
       "\n",
       "          com_body_pos_count_(-3)  com_body_very_pos_count_(-3)  \\\n",
       "date                                                              \n",
       "1/1/2014                   1347.0                         649.0   \n",
       "1/2/2014                   1547.0                         783.0   \n",
       "1/3/2014                   1162.0                         591.0   \n",
       "1/4/2014                    997.0                         496.0   \n",
       "1/5/2014                   1351.0                         619.0   \n",
       "\n",
       "          com_body_neg_count_(-3)  com_body_very_neg_count_(-3)  \\\n",
       "date                                                              \n",
       "1/1/2014                    615.0                         245.0   \n",
       "1/2/2014                    659.0                         277.0   \n",
       "1/3/2014                    577.0                         224.0   \n",
       "1/4/2014                    468.0                         172.0   \n",
       "1/5/2014                    533.0                         212.0   \n",
       "\n",
       "          sub_body_pos_count_(-3)  sub_body_very_pos_count_(-3)  \\\n",
       "date                                                              \n",
       "1/1/2014                     92.0                          67.0   \n",
       "1/2/2014                     96.0                          65.0   \n",
       "1/3/2014                     90.0                          77.0   \n",
       "1/4/2014                     63.0                          47.0   \n",
       "1/5/2014                     96.0                          67.0   \n",
       "\n",
       "          sub_body_neg_count_(-3)  sub_body_very_neg_count_(-3)  \\\n",
       "date                                                              \n",
       "1/1/2014                     17.0                           4.0   \n",
       "1/2/2014                     31.0                          19.0   \n",
       "1/3/2014                     19.0                           7.0   \n",
       "1/4/2014                     17.0                           9.0   \n",
       "1/5/2014                     18.0                          12.0   \n",
       "\n",
       "          sub_title_pos_count_(-3)  sub_title_very_pos_count_(-3)  \\\n",
       "date                                                                \n",
       "1/1/2014                      43.0                           10.0   \n",
       "1/2/2014                      48.0                           14.0   \n",
       "1/3/2014                      50.0                           11.0   \n",
       "1/4/2014                      42.0                            8.0   \n",
       "1/5/2014                      73.0                           12.0   \n",
       "\n",
       "          sub_title_neg_count_(-3)  sub_title_very_neg_count_(-3)  \\\n",
       "date                                                                \n",
       "1/1/2014                      25.0                            3.0   \n",
       "1/2/2014                      27.0                            5.0   \n",
       "1/3/2014                      20.0                            5.0   \n",
       "1/4/2014                      13.0                            3.0   \n",
       "1/5/2014                      21.0                            2.0   \n",
       "\n",
       "          avg_clust_coef_(-3)  avg_degree_(-3)  num_edges_(-3)  \\\n",
       "date                                                             \n",
       "1/1/2014             0.007134             1.30          2436.0   \n",
       "1/2/2014             0.004985             1.42          2762.0   \n",
       "1/3/2014             0.003238             1.25          2234.0   \n",
       "1/4/2014             0.006140             1.20          1900.0   \n",
       "1/5/2014             0.005718             1.28          2458.0   \n",
       "\n",
       "          num_nodes_(-3)  bitcoin_open_(-3)  bitcoin_high_(-3)  \\\n",
       "date                                                             \n",
       "1/1/2014          1877.0             728.05             748.61   \n",
       "1/2/2014          1944.0             741.35             766.60   \n",
       "1/3/2014          1789.0             760.32             760.58   \n",
       "1/4/2014          1581.0             754.97             775.35   \n",
       "1/5/2014          1913.0             773.44             820.31   \n",
       "\n",
       "          bitcoin_low_(-3)  bitcoin_close_(-3)  bitcoin_volume_(-3)  \\\n",
       "date                                                                  \n",
       "1/1/2014            714.44              745.05           19011300.0   \n",
       "1/2/2014            740.24              756.13           20707700.0   \n",
       "1/3/2014            738.17              754.01           20897300.0   \n",
       "1/4/2014            754.97              771.40           22489400.0   \n",
       "1/5/2014            767.21              802.39           38489500.0   \n",
       "\n",
       "          bitcoin_market_cap_(-3)  com_count_(-4)  sub_count_(-4)  \\\n",
       "date                                                                \n",
       "1/1/2014             8.872600e+09          5335.0           354.0   \n",
       "1/2/2014             9.037000e+09          5413.0           341.0   \n",
       "1/3/2014             9.268240e+09          5757.0           447.0   \n",
       "1/4/2014             9.203030e+09          4772.0           352.0   \n",
       "1/5/2014             9.428180e+09          4173.0           288.0   \n",
       "\n",
       "          com_body_pos_count_(-4)  com_body_very_pos_count_(-4)  \\\n",
       "date                                                              \n",
       "1/1/2014                   1364.0                         659.0   \n",
       "1/2/2014                   1347.0                         649.0   \n",
       "1/3/2014                   1547.0                         783.0   \n",
       "1/4/2014                   1162.0                         591.0   \n",
       "1/5/2014                    997.0                         496.0   \n",
       "\n",
       "          com_body_neg_count_(-4)  com_body_very_neg_count_(-4)  \\\n",
       "date                                                              \n",
       "1/1/2014                    565.0                         226.0   \n",
       "1/2/2014                    615.0                         245.0   \n",
       "1/3/2014                    659.0                         277.0   \n",
       "1/4/2014                    577.0                         224.0   \n",
       "1/5/2014                    468.0                         172.0   \n",
       "\n",
       "          sub_body_pos_count_(-4)  sub_body_very_pos_count_(-4)  \\\n",
       "date                                                              \n",
       "1/1/2014                     85.0                          63.0   \n",
       "1/2/2014                     92.0                          67.0   \n",
       "1/3/2014                     96.0                          65.0   \n",
       "1/4/2014                     90.0                          77.0   \n",
       "1/5/2014                     63.0                          47.0   \n",
       "\n",
       "          sub_body_neg_count_(-4)  sub_body_very_neg_count_(-4)  \\\n",
       "date                                                              \n",
       "1/1/2014                     23.0                          14.0   \n",
       "1/2/2014                     17.0                           4.0   \n",
       "1/3/2014                     31.0                          19.0   \n",
       "1/4/2014                     19.0                           7.0   \n",
       "1/5/2014                     17.0                           9.0   \n",
       "\n",
       "          sub_title_pos_count_(-4)  sub_title_very_pos_count_(-4)  \\\n",
       "date                                                                \n",
       "1/1/2014                      54.0                           14.0   \n",
       "1/2/2014                      43.0                           10.0   \n",
       "1/3/2014                      48.0                           14.0   \n",
       "1/4/2014                      50.0                           11.0   \n",
       "1/5/2014                      42.0                            8.0   \n",
       "\n",
       "          sub_title_neg_count_(-4)  sub_title_very_neg_count_(-4)  \\\n",
       "date                                                                \n",
       "1/1/2014                      21.0                            5.0   \n",
       "1/2/2014                      25.0                            3.0   \n",
       "1/3/2014                      27.0                            5.0   \n",
       "1/4/2014                      20.0                            5.0   \n",
       "1/5/2014                      13.0                            3.0   \n",
       "\n",
       "          avg_clust_coef_(-4)  avg_degree_(-4)  num_edges_(-4)  \\\n",
       "date                                                             \n",
       "1/1/2014             0.004497             1.43          2689.0   \n",
       "1/2/2014             0.007134             1.30          2436.0   \n",
       "1/3/2014             0.004985             1.42          2762.0   \n",
       "1/4/2014             0.003238             1.25          2234.0   \n",
       "1/5/2014             0.006140             1.20          1900.0   \n",
       "\n",
       "          num_nodes_(-4)  bitcoin_open_(-4)  bitcoin_high_(-4)  \\\n",
       "date                                                             \n",
       "1/1/2014          1883.0             737.98             747.06   \n",
       "1/2/2014          1877.0             728.05             748.61   \n",
       "1/3/2014          1944.0             741.35             766.60   \n",
       "1/4/2014          1789.0             760.32             760.58   \n",
       "1/5/2014          1581.0             754.97             775.35   \n",
       "\n",
       "          bitcoin_low_(-4)  bitcoin_close_(-4)  bitcoin_volume_(-4)  \\\n",
       "date                                                                  \n",
       "1/1/2014            705.35              727.83           32505800.0   \n",
       "1/2/2014            714.44              745.05           19011300.0   \n",
       "1/3/2014            740.24              756.13           20707700.0   \n",
       "1/4/2014            738.17              754.01           20897300.0   \n",
       "1/5/2014            754.97              771.40           22489400.0   \n",
       "\n",
       "          bitcoin_market_cap_(-4)  com_count_(-5)  sub_count_(-5)  \\\n",
       "date                                                                \n",
       "1/1/2014             8.990850e+09          6065.0           477.0   \n",
       "1/2/2014             8.872600e+09          5335.0           354.0   \n",
       "1/3/2014             9.037000e+09          5413.0           341.0   \n",
       "1/4/2014             9.268240e+09          5757.0           447.0   \n",
       "1/5/2014             9.203030e+09          4772.0           352.0   \n",
       "\n",
       "          com_body_pos_count_(-5)  com_body_very_pos_count_(-5)  \\\n",
       "date                                                              \n",
       "1/1/2014                   1516.0                         766.0   \n",
       "1/2/2014                   1364.0                         659.0   \n",
       "1/3/2014                   1347.0                         649.0   \n",
       "1/4/2014                   1547.0                         783.0   \n",
       "1/5/2014                   1162.0                         591.0   \n",
       "\n",
       "          com_body_neg_count_(-5)  com_body_very_neg_count_(-5)  \\\n",
       "date                                                              \n",
       "1/1/2014                    666.0                         253.0   \n",
       "1/2/2014                    565.0                         226.0   \n",
       "1/3/2014                    615.0                         245.0   \n",
       "1/4/2014                    659.0                         277.0   \n",
       "1/5/2014                    577.0                         224.0   \n",
       "\n",
       "          sub_body_pos_count_(-5)  sub_body_very_pos_count_(-5)  \\\n",
       "date                                                              \n",
       "1/1/2014                    105.0                          74.0   \n",
       "1/2/2014                     85.0                          63.0   \n",
       "1/3/2014                     92.0                          67.0   \n",
       "1/4/2014                     96.0                          65.0   \n",
       "1/5/2014                     90.0                          77.0   \n",
       "\n",
       "          sub_body_neg_count_(-5)  sub_body_very_neg_count_(-5)  \\\n",
       "date                                                              \n",
       "1/1/2014                     14.0                           6.0   \n",
       "1/2/2014                     23.0                          14.0   \n",
       "1/3/2014                     17.0                           4.0   \n",
       "1/4/2014                     31.0                          19.0   \n",
       "1/5/2014                     19.0                           7.0   \n",
       "\n",
       "          sub_title_pos_count_(-5)  sub_title_very_pos_count_(-5)  \\\n",
       "date                                                                \n",
       "1/1/2014                      67.0                           10.0   \n",
       "1/2/2014                      54.0                           14.0   \n",
       "1/3/2014                      43.0                           10.0   \n",
       "1/4/2014                      48.0                           14.0   \n",
       "1/5/2014                      50.0                           11.0   \n",
       "\n",
       "          sub_title_neg_count_(-5)  sub_title_very_neg_count_(-5)  \\\n",
       "date                                                                \n",
       "1/1/2014                      22.0                            4.0   \n",
       "1/2/2014                      21.0                            5.0   \n",
       "1/3/2014                      25.0                            3.0   \n",
       "1/4/2014                      27.0                            5.0   \n",
       "1/5/2014                      20.0                            5.0   \n",
       "\n",
       "          avg_clust_coef_(-5)  avg_degree_(-5)  num_edges_(-5)  \\\n",
       "date                                                             \n",
       "1/1/2014             0.006916             1.41          2856.0   \n",
       "1/2/2014             0.004497             1.43          2689.0   \n",
       "1/3/2014             0.007134             1.30          2436.0   \n",
       "1/4/2014             0.004985             1.42          2762.0   \n",
       "1/5/2014             0.003238             1.25          2234.0   \n",
       "\n",
       "          num_nodes_(-5)  bitcoin_open_(-5)  bitcoin_high_(-5)  \\\n",
       "date                                                             \n",
       "1/1/2014          2019.0             763.28             777.51   \n",
       "1/2/2014          1883.0             737.98             747.06   \n",
       "1/3/2014          1877.0             728.05             748.61   \n",
       "1/4/2014          1944.0             741.35             766.60   \n",
       "1/5/2014          1789.0             760.32             760.58   \n",
       "\n",
       "          bitcoin_low_(-5)  bitcoin_close_(-5)  bitcoin_volume_(-5)  \\\n",
       "date                                                                  \n",
       "1/1/2014            713.60              735.07           46862700.0   \n",
       "1/2/2014            705.35              727.83           32505800.0   \n",
       "1/3/2014            714.44              745.05           19011300.0   \n",
       "1/4/2014            740.24              756.13           20707700.0   \n",
       "1/5/2014            738.17              754.01           20897300.0   \n",
       "\n",
       "          bitcoin_market_cap_(-5)  y  \n",
       "date                                  \n",
       "1/1/2014             9.295570e+09  1  \n",
       "1/2/2014             8.990850e+09  1  \n",
       "1/3/2014             8.872600e+09  1  \n",
       "1/4/2014             9.037000e+09  1  \n",
       "1/5/2014             9.268240e+09  1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    947\n",
       "0    811\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# balance of dependent variable values\n",
    "df.y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting For Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create train/test X and y\n",
    "def create_training_test_splits(df_in, train_split=0.7, rescale=True):\n",
    "    # split into test train\n",
    "    X, y = df_in[df_in.columns[:-1]], df_in[df_in.columns[-1]]\n",
    "    \n",
    "    # determine cutoff of train/test split and split\n",
    "    cutoff = int(X.shape[0] * train_split)\n",
    "    X_train, y_train = X[:cutoff], y[:cutoff]\n",
    "    X_test, y_test = X[cutoff:], y[cutoff:]\n",
    "    \n",
    "    # scale values to 0-1\n",
    "    # must only scale based on training data, to ensure no \n",
    "    # foresight with averages\n",
    "    if rescale:\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        cols = X.columns\n",
    "        X_train[X_train.columns] = scaler.fit_transform(X_train[X_train.columns])\n",
    "        X_test[X_test.columns] = scaler.transform(X_test[X_test.columns])\n",
    "        \n",
    "    # extract values of X and y\n",
    "    X_train, X_test = X_train.values, X_test.values\n",
    "    y_train, y_test = y_train.values, y_test.values\n",
    "    \n",
    "    # reshape input to be LSTM format [samples, timesteps, features]\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\CryptoProject\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Anaconda3\\envs\\CryptoProject\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\envs\\CryptoProject\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "C:\\Anaconda3\\envs\\CryptoProject\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\envs\\CryptoProject\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = create_training_test_splits(df, train_split=0.8, rescale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Train, Test LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create and train model\n",
    "\n",
    "def run_model(lstm_neurs=100, dense_neurs=10, dropout_lstm=0.2, dropout_dense1=0.2, dropout_dense2=0.2,\\\n",
    "              dropout_recurr=0.2, l2_lstm=0.01, l2_dense=0.01, batch_size=14, \\\n",
    "              kernel_initializer='glorot_uniform', epochs=10):\n",
    "    # create LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_neurs, kernel_initializer=kernel_initializer, \n",
    "                   input_shape=(X_train.shape[1], X_train.shape[2]), \n",
    "                   dropout=dropout_lstm, recurrent_dropout=dropout_recurr,\n",
    "                    kernel_regularizer=regularizers.l2(l2_lstm)))\n",
    "    model.add(Dropout(dropout_dense1))\n",
    "    model.add(Dense(dense_neurs, activation='relu', kernel_regularizer=regularizers.l2(l2_dense)))\n",
    "    model.add(Dropout(dropout_dense2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # compile model and output summary \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    # fit model and evaluate\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,shuffle=False)\n",
    "    scores = model.evaluate(X_test, y_test, verbose=2)\n",
    "    y_pred = np.round(model.predict(X_test))\n",
    "    accuracy = scores[1] * 100\n",
    "    print(\"\\nAccuracy: {:.2f}%\".format(accuracy))\n",
    "    \n",
    "    # print confusion matrix\n",
    "    print('\\nConfusion Matrix')\n",
    "    print('='*20)\n",
    "    print(pd.DataFrame(confusion_matrix(y_test, y_pred), \n",
    "                       index=['true:no', 'true:yes'], columns=['pred:no', 'pred:yes']))\n",
    "    \n",
    "    # print classification report\n",
    "    print('\\nClasification Report')\n",
    "    print('='*20)\n",
    "    print(classification_report(y_test, y_pred, labels=[0,1]))\n",
    "    \n",
    "    return model, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               98000     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 99,021\n",
      "Trainable params: 99,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "1406/1406 [==============================] - 3s 2ms/step - loss: 1.9799 - acc: 0.5164\n",
      "Epoch 2/10\n",
      "1406/1406 [==============================] - 0s 213us/step - loss: 0.9757 - acc: 0.5405\n",
      "Epoch 3/10\n",
      "1406/1406 [==============================] - 0s 222us/step - loss: 0.7553 - acc: 0.5405\n",
      "Epoch 4/10\n",
      "1406/1406 [==============================] - 0s 234us/step - loss: 0.7097 - acc: 0.5405\n",
      "Epoch 5/10\n",
      "1406/1406 [==============================] - 0s 204us/step - loss: 0.6994 - acc: 0.5405\n",
      "Epoch 6/10\n",
      "1406/1406 [==============================] - 0s 212us/step - loss: 0.6936 - acc: 0.5405\n",
      "Epoch 7/10\n",
      "1406/1406 [==============================] - 0s 249us/step - loss: 0.6917 - acc: 0.5405\n",
      "Epoch 8/10\n",
      "1406/1406 [==============================] - 0s 208us/step - loss: 0.6904 - acc: 0.5405\n",
      "Epoch 9/10\n",
      "1406/1406 [==============================] - 0s 207us/step - loss: 0.6907 - acc: 0.5405\n",
      "Epoch 10/10\n",
      "1406/1406 [==============================] - 0s 230us/step - loss: 0.6908 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\CryptoProject\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model,acc  = run_model(lstm_neurs=100, dense_neurs=10, dropout_lstm=0.2, dropout_dense1=0.2, dropout_dense2=0.2,\\\n",
    "              dropout_recurr=0.2, l2_lstm=0.01, l2_dense=0.01, batch_size=30, \\\n",
    "              kernel_initializer='glorot_uniform', epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Training and LSTM Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulations(uniques=True, timer=True):\n",
    "    # run all simulations to find best model\n",
    "    if timer:\n",
    "        start = time.time()\n",
    "    \n",
    "    # number of lags\n",
    "    n_lags = [0, 7, 14]\n",
    "    # train/test splits\n",
    "    train_splits = [0.75, 0.9]\n",
    "    \n",
    "    # neurons\n",
    "    # number of neurons in LSTM\n",
    "    lstm_neurons = [10, 50, 100]\n",
    "    # number of neurons in Dense\n",
    "    dense_neurons = [5, 10]\n",
    "    \n",
    "    # dropouts\n",
    "    # dropout of LSTM layer\n",
    "    dropout_lstm = [0.1, 0.25]\n",
    "    # dropout of layer before Dense layer\n",
    "    dropout_dense1 = [0.25]\n",
    "    # dropout of layer after Dense layer\n",
    "    dropout_dense2 = [0.25]\n",
    "    # dropout in recurrent layer of LSTM\n",
    "    dropout_recurr = [0.1]\n",
    "    \n",
    "    # training and initializers\n",
    "    # batch size of iterations\n",
    "    batch_size = [7, 30]\n",
    "    # kernel initializers for LSTM layer\n",
    "    kernel_initializer=['glorot_uniform', 'glorot_normal']\n",
    "    \n",
    "    # regularization\n",
    "    # regularization penalties for LSTM layer\n",
    "    l2s_lstms = [0.1]\n",
    "    # regularization penalties for Dense layer\n",
    "    l2s_denses = [0.25]\n",
    "    \n",
    "    # misc\n",
    "    # epochs for iterations\n",
    "    epochs = [5]\n",
    "    # dashes for formatting\n",
    "    dashes = 65\n",
    "    \n",
    "    # RUN\n",
    "    # number of reports to keep track\n",
    "    report = 0\n",
    "    accs = set()\n",
    "    \n",
    "    # data preparation parameters\n",
    "    for lag in n_lags:\n",
    "        # read in data and create lags\n",
    "        df = prepare_data(n_lags=lag)\n",
    "        for split in train_splits:\n",
    "            # create training and test data\n",
    "            X_train, X_test, y_train, y_test = create_training_test_splits(\n",
    "                                    df, train_split=split, rescale=True)\n",
    "\n",
    "            # run all simulations of for modeling parameters\n",
    "            for lstm_n in lstm_neurons:\n",
    "                for dense_n in dense_neurons:\n",
    "                    for d_lstm in dropout_lstm:\n",
    "                        for d_dense1 in dropout_dense1:\n",
    "                            for d_dense2 in dropout_dense2:\n",
    "                                for d_recurr in dropout_recurr:\n",
    "                                    for l2_lstm in l2s_lstms:\n",
    "                                        for l2_dense in l2s_denses:\n",
    "                                            for batch in batch_size:\n",
    "                                                for k_init in kernel_initializer:\n",
    "                                                    for e in epochs:\n",
    "                                                        # print output\n",
    "                                                        report += 1\n",
    "                                                        print('='*dashes)\n",
    "                                                        print('REPORT # {}'.format(report))\n",
    "                                                        print('='*dashes)\n",
    "                                                        print('Training Parameters')\n",
    "                                                        print('Number of lags    = {}'.format(lag))\n",
    "                                                        print('Training Data %   = {:0.1f}%'.format(split*100))\n",
    "                                                        print('='*dashes)\n",
    "                                                        print('Model Parameters')\n",
    "                                                        print('LSTM Neurons      = {}'.format(lstm_n))\n",
    "                                                        print('Dense Neurons     = {}'.format(dense_n))\n",
    "                                                        print('Dropout LSTM      = {}'.format(d_lstm))\n",
    "                                                        print('Dropout Dense1    = {}'.format(d_dense1))\n",
    "                                                        print('Dropout Dense2    = {}'.format(d_dense2))\n",
    "                                                        print('Dropout Recurrent = {}'.format(d_recurr))\n",
    "                                                        print('L2 - LSTM Layer   = {}'.format(l2_lstm))\n",
    "                                                        print('L2 - Dense Layer  = {}'.format(l2_dense))\n",
    "                                                        print('Batch Size        = {}'.format(batch))\n",
    "                                                        print('Kernel Init.      = {}'.format(k_init))\n",
    "                                                        print('Epochs            = {}'.format(e))\n",
    "                                                        print('='*dashes)\n",
    "                                                        # execute model\n",
    "                                                        model, acc = run_model(lstm_neurs=lstm_n, dense_neurs=dense_n, \\\n",
    "                                                                  dropout_lstm=d_lstm, dropout_dense1=d_dense1, \\\n",
    "                                                                  dropout_dense2=d_dense2, dropout_recurr=d_recurr, \\\n",
    "                                                                  l2_lstm=l2_lstm, l2_dense=l2_dense, batch_size=batch, \\\n",
    "                                                                  kernel_initializer=k_init, epochs=e)\n",
    "                                                        # update minimum accuracy and skip over\n",
    "                                                        # any models with same as previous accuracy scores\n",
    "                                                        if acc in accs and uniques: continue\n",
    "                                                        model.save('models/model-{}_acc-{}.h5'.format( \\\n",
    "                                                                report, round(acc)))\n",
    "                                                        accs.add(acc)\n",
    "                                                        print('='*dashes)\n",
    "    if timer:\n",
    "        end = time.time()\n",
    "        print('Process took {:.2f} seconds'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\CryptoProject\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Anaconda3\\envs\\CryptoProject\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\envs\\CryptoProject\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "C:\\Anaconda3\\envs\\CryptoProject\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\envs\\CryptoProject\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "REPORT # 1\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 10\n",
      "Dense Neurons     = 5\n",
      "Dropout LSTM      = 0.1\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 7\n",
      "Kernel Init.      = glorot_uniform\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 10)                6200      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 6,261\n",
      "Trainable params: 6,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 2s 2ms/step - loss: 3.3228 - acc: 0.5398\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 1s 893us/step - loss: 1.0420 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 1s 923us/step - loss: 0.7911 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 1s 908us/step - loss: 0.7159 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 1s 894us/step - loss: 0.6947 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\CryptoProject\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "=================================================================\n",
      "REPORT # 2\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 10\n",
      "Dense Neurons     = 5\n",
      "Dropout LSTM      = 0.1\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 7\n",
      "Kernel Init.      = glorot_normal\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 10)                6200      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 6,261\n",
      "Trainable params: 6,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 2s 2ms/step - loss: 3.5301 - acc: 0.5384\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 1s 898us/step - loss: 1.0949 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 1s 923us/step - loss: 0.8084 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 1s 904us/step - loss: 0.7222 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 1s 902us/step - loss: 0.6968 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 3\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 10\n",
      "Dense Neurons     = 5\n",
      "Dropout LSTM      = 0.1\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 30\n",
      "Kernel Init.      = glorot_uniform\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 10)                6200      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 6,261\n",
      "Trainable params: 6,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 2s 1ms/step - loss: 6.4527 - acc: 0.4716\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 0s 213us/step - loss: 3.5699 - acc: 0.5427\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 0s 244us/step - loss: 2.2129 - acc: 0.5398\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 0s 214us/step - loss: 1.5791 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 0s 210us/step - loss: 1.2680 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 4\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 10\n",
      "Dense Neurons     = 5\n",
      "Dropout LSTM      = 0.1\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 30\n",
      "Kernel Init.      = glorot_normal\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 10)                6200      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 6,261\n",
      "Trainable params: 6,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 2s 1ms/step - loss: 6.5718 - acc: 0.5334\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 0s 228us/step - loss: 3.8437 - acc: 0.5420\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 0s 230us/step - loss: 2.4872 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 0s 205us/step - loss: 1.7857 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 0s 217us/step - loss: 1.4058 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 5\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 10\n",
      "Dense Neurons     = 5\n",
      "Dropout LSTM      = 0.25\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 7\n",
      "Kernel Init.      = glorot_uniform\n",
      "Epochs            = 5\n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 10)                6200      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 6,261\n",
      "Trainable params: 6,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 3s 2ms/step - loss: 3.3357 - acc: 0.5249\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 1s 916us/step - loss: 1.0523 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 1s 912us/step - loss: 0.7939 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 1s 950us/step - loss: 0.7161 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 1s 923us/step - loss: 0.6955 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 6\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 10\n",
      "Dense Neurons     = 5\n",
      "Dropout LSTM      = 0.25\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 7\n",
      "Kernel Init.      = glorot_normal\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 10)                6200      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 6,261\n",
      "Trainable params: 6,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 3s 2ms/step - loss: 3.5819 - acc: 0.5341\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 1s 960us/step - loss: 1.1281 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 1s 926us/step - loss: 0.8119 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 1s 925us/step - loss: 0.7227 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 1s 957us/step - loss: 0.6970 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 7\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 10\n",
      "Dense Neurons     = 5\n",
      "Dropout LSTM      = 0.25\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 30\n",
      "Kernel Init.      = glorot_uniform\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 10)                6200      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 6,261\n",
      "Trainable params: 6,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 2s 1ms/step - loss: 6.8813 - acc: 0.4737\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 0s 250us/step - loss: 3.9795 - acc: 0.5455\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 0s 211us/step - loss: 2.5882 - acc: 0.5398\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 0s 210us/step - loss: 1.9139 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 0s 247us/step - loss: 1.5608 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 8\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 10\n",
      "Dense Neurons     = 5\n",
      "Dropout LSTM      = 0.25\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 30\n",
      "Kernel Init.      = glorot_normal\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 10)                6200      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 6,261\n",
      "Trainable params: 6,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 2s 2ms/step - loss: 6.3694 - acc: 0.5228\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 0s 237us/step - loss: 3.6910 - acc: 0.5391\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 0s 210us/step - loss: 2.3741 - acc: 0.5398\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 0s 218us/step - loss: 1.7004 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 0s 242us/step - loss: 1.3380 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 9\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 10\n",
      "Dense Neurons     = 10\n",
      "Dropout LSTM      = 0.1\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 7\n",
      "Kernel Init.      = glorot_uniform\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, 10)                6200      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 6,321\n",
      "Trainable params: 6,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 3s 2ms/step - loss: 4.1974 - acc: 0.5363\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 1s 921us/step - loss: 1.2769 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 1s 929us/step - loss: 0.8357 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 1s 947us/step - loss: 0.7191 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 1s 935us/step - loss: 0.6945 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 10\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 10\n",
      "Dense Neurons     = 10\n",
      "Dropout LSTM      = 0.1\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 7\n",
      "Kernel Init.      = glorot_normal\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_11 (LSTM)               (None, 10)                6200      \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 6,321\n",
      "Trainable params: 6,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 3s 2ms/step - loss: 4.3224 - acc: 0.5149\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 1s 972us/step - loss: 1.2884 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 1s 926us/step - loss: 0.8278 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 1s 935us/step - loss: 0.7169 - acc: 0.5405 1s - loss\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 1s 960us/step - loss: 0.6945 - acc: 0.5405 1s - l\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 11\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 10\n",
      "Dense Neurons     = 10\n",
      "Dropout LSTM      = 0.1\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 30\n",
      "Kernel Init.      = glorot_uniform\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 10)                6200      \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 6,321\n",
      "Trainable params: 6,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 3s 2ms/step - loss: 7.4862 - acc: 0.4623\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 0s 257us/step - loss: 4.3875 - acc: 0.5334\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 0s 265us/step - loss: 2.8476 - acc: 0.5405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 0s 216us/step - loss: 2.0635 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 0s 220us/step - loss: 1.6331 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 12\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 10\n",
      "Dense Neurons     = 10\n",
      "Dropout LSTM      = 0.1\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 30\n",
      "Kernel Init.      = glorot_normal\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 10)                6200      \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 6,321\n",
      "Trainable params: 6,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 3s 2ms/step - loss: 7.3370 - acc: 0.4765\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 0s 250us/step - loss: 4.3718 - acc: 0.5441\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 0s 214us/step - loss: 2.8490 - acc: 0.5413\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 0s 215us/step - loss: 2.0312 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 0s 247us/step - loss: 1.5688 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 13\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 10\n",
      "Dense Neurons     = 10\n",
      "Dropout LSTM      = 0.25\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 7\n",
      "Kernel Init.      = glorot_uniform\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_14 (LSTM)               (None, 10)                6200      \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 6,321\n",
      "Trainable params: 6,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 4s 3ms/step - loss: 3.9012 - acc: 0.5299\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 1s 953us/step - loss: 1.1657 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 1s 942us/step - loss: 0.8076 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 1s 977us/step - loss: 0.7143 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 1s 946us/step - loss: 0.6940 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 14\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 10\n",
      "Dense Neurons     = 10\n",
      "Dropout LSTM      = 0.25\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 7\n",
      "Kernel Init.      = glorot_normal\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_15 (LSTM)               (None, 10)                6200      \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 6,321\n",
      "Trainable params: 6,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 4s 3ms/step - loss: 4.2945 - acc: 0.5363\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 1s 953us/step - loss: 1.2705 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 1s 968us/step - loss: 0.8236 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 1s 945us/step - loss: 0.7167 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 1s 971us/step - loss: 0.6949 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 15\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 10\n",
      "Dense Neurons     = 10\n",
      "Dropout LSTM      = 0.25\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 30\n",
      "Kernel Init.      = glorot_uniform\n",
      "Epochs            = 5\n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_16 (LSTM)               (None, 10)                6200      \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 6,321\n",
      "Trainable params: 6,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 3s 2ms/step - loss: 7.1133 - acc: 0.5235\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 0s 232us/step - loss: 4.0655 - acc: 0.5434\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 0s 247us/step - loss: 2.5802 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 0s 220us/step - loss: 1.8464 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 0s 229us/step - loss: 1.4584 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 16\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 10\n",
      "Dense Neurons     = 10\n",
      "Dropout LSTM      = 0.25\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 30\n",
      "Kernel Init.      = glorot_normal\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_17 (LSTM)               (None, 10)                6200      \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 6,321\n",
      "Trainable params: 6,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 3s 2ms/step - loss: 7.2481 - acc: 0.4587\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 0s 218us/step - loss: 4.3278 - acc: 0.5249\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 0s 241us/step - loss: 2.8217 - acc: 0.5413\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 0s 228us/step - loss: 2.0080 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 0s 217us/step - loss: 1.5450 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 17\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 50\n",
      "Dense Neurons     = 5\n",
      "Dropout LSTM      = 0.1\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 7\n",
      "Kernel Init.      = glorot_uniform\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_18 (LSTM)               (None, 50)                39000     \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 5)                 255       \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 39,261\n",
      "Trainable params: 39,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 4s 3ms/step - loss: 4.9623 - acc: 0.5377\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 1s 990us/step - loss: 0.8183 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 1s 985us/step - loss: 0.6994 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 1s 971us/step - loss: 0.6907 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 1s 994us/step - loss: 0.6902 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 18\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 50\n",
      "Dense Neurons     = 5\n",
      "Dropout LSTM      = 0.1\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 7\n",
      "Kernel Init.      = glorot_normal\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_19 (LSTM)               (None, 50)                39000     \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 5)                 255       \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 39,261\n",
      "Trainable params: 39,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 4s 3ms/step - loss: 5.4708 - acc: 0.5427\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 1s 956us/step - loss: 0.8333 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 1s 964us/step - loss: 0.6981 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 1s 983us/step - loss: 0.6910 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 1s 953us/step - loss: 0.6906 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 19\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 50\n",
      "Dense Neurons     = 5\n",
      "Dropout LSTM      = 0.1\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 30\n",
      "Kernel Init.      = glorot_uniform\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_20 (LSTM)               (None, 50)                39000     \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 5)                 255       \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 39,261\n",
      "Trainable params: 39,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 3s 2ms/step - loss: 12.5594 - acc: 0.4922\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 0s 222us/step - loss: 4.4978 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 0s 228us/step - loss: 1.9201 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 0s 248us/step - loss: 1.1519 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 0s 223us/step - loss: 0.9035 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 20\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 50\n",
      "Dense Neurons     = 5\n",
      "Dropout LSTM      = 0.1\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 30\n",
      "Kernel Init.      = glorot_normal\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_21 (LSTM)               (None, 50)                39000     \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 5)                 255       \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 39,261\n",
      "Trainable params: 39,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 4s 3ms/step - loss: 12.9106 - acc: 0.4822\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 0s 227us/step - loss: 5.2223 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 0s 223us/step - loss: 2.4395 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 0s 254us/step - loss: 1.4114 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 0s 229us/step - loss: 1.0148 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 21\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 50\n",
      "Dense Neurons     = 5\n",
      "Dropout LSTM      = 0.25\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 7\n",
      "Kernel Init.      = glorot_uniform\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_22 (LSTM)               (None, 50)                39000     \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 5)                 255       \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 39,261\n",
      "Trainable params: 39,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 5s 3ms/step - loss: 4.8872 - acc: 0.5341\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 1s 988us/step - loss: 0.8022 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 1s 1ms/step - loss: 0.6980 - acc: 0.5405\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1406/1406 [==============================] - 1s 965us/step - loss: 0.6907 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 1s 994us/step - loss: 0.6902 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 22\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 50\n",
      "Dense Neurons     = 5\n",
      "Dropout LSTM      = 0.25\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 7\n",
      "Kernel Init.      = glorot_normal\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_23 (LSTM)               (None, 50)                39000     \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 5)                 255       \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 39,261\n",
      "Trainable params: 39,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 5s 4ms/step - loss: 5.4420 - acc: 0.5469\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 1s 1ms/step - loss: 0.8238 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 1s 987us/step - loss: 0.6969 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 1s 1ms/step - loss: 0.6905 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 1s 1ms/step - loss: 0.6902 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 23\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 50\n",
      "Dense Neurons     = 5\n",
      "Dropout LSTM      = 0.25\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 30\n",
      "Kernel Init.      = glorot_uniform\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_24 (LSTM)               (None, 50)                39000     \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 5)                 255       \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 39,261\n",
      "Trainable params: 39,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 4s 3ms/step - loss: 12.7220 - acc: 0.5028\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 0s 264us/step - loss: 4.6422 - acc: 0.5384\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 0s 230us/step - loss: 2.0327 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 0s 258us/step - loss: 1.2307 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 0s 260us/step - loss: 0.9561 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 24\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 50\n",
      "Dense Neurons     = 5\n",
      "Dropout LSTM      = 0.25\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 30\n",
      "Kernel Init.      = glorot_normal\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_25 (LSTM)               (None, 50)                39000     \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 5)                 255       \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 39,261\n",
      "Trainable params: 39,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 4s 3ms/step - loss: 13.2980 - acc: 0.5284\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 0s 258us/step - loss: 5.4762 - acc: 0.5434\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 0s 245us/step - loss: 2.6051 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 0s 227us/step - loss: 1.5184 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 0s 262us/step - loss: 1.0836 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 25\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 50\n",
      "Dense Neurons     = 10\n",
      "Dropout LSTM      = 0.1\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 7\n",
      "Kernel Init.      = glorot_uniform\n",
      "Epochs            = 5\n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_26 (LSTM)               (None, 50)                39000     \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 39,521\n",
      "Trainable params: 39,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 5s 4ms/step - loss: 5.7616 - acc: 0.5341\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 1s 1ms/step - loss: 0.8723 - acc: 0.5405A: 1s - \n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 1s 1ms/step - loss: 0.7001 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 2s 1ms/step - loss: 0.6908 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 1s 1ms/step - loss: 0.6900 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 26\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 50\n",
      "Dense Neurons     = 10\n",
      "Dropout LSTM      = 0.1\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 7\n",
      "Kernel Init.      = glorot_normal\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_27 (LSTM)               (None, 50)                39000     \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 39,521\n",
      "Trainable params: 39,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 6s 4ms/step - loss: 6.0029 - acc: 0.5235\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 1s 1ms/step - loss: 0.8733 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 1s 1ms/step - loss: 0.7001 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 2s 1ms/step - loss: 0.6906 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 1s 1ms/step - loss: 0.6904 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 27\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 50\n",
      "Dense Neurons     = 10\n",
      "Dropout LSTM      = 0.1\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 30\n",
      "Kernel Init.      = glorot_uniform\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_28 (LSTM)               (None, 50)                39000     \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 39,521\n",
      "Trainable params: 39,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 5s 3ms/step - loss: 14.4266 - acc: 0.5057\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 0s 252us/step - loss: 5.6532 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 0s 260us/step - loss: 2.6153 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 0s 262us/step - loss: 1.5585 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 0s 239us/step - loss: 1.1349 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 28\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 50\n",
      "Dense Neurons     = 10\n",
      "Dropout LSTM      = 0.1\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 30\n",
      "Kernel Init.      = glorot_normal\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_29 (LSTM)               (None, 50)                39000     \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 39,521\n",
      "Trainable params: 39,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 5s 3ms/step - loss: 14.5554 - acc: 0.5000\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 0s 241us/step - loss: 6.1436 - acc: 0.5420\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 0s 238us/step - loss: 2.9559 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 0s 271us/step - loss: 1.6989 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 0s 232us/step - loss: 1.1714 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 29\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 50\n",
      "Dense Neurons     = 10\n",
      "Dropout LSTM      = 0.25\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 7\n",
      "Kernel Init.      = glorot_uniform\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_30 (LSTM)               (None, 50)                39000     \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 39,521\n",
      "Trainable params: 39,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 6s 4ms/step - loss: 5.6353 - acc: 0.5327\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 1s 1ms/step - loss: 0.8641 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 1s 1ms/step - loss: 0.6999 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 1s 1ms/step - loss: 0.6916 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 2s 1ms/step - loss: 0.6907 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 30\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 50\n",
      "Dense Neurons     = 10\n",
      "Dropout LSTM      = 0.25\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 7\n",
      "Kernel Init.      = glorot_normal\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_31 (LSTM)               (None, 50)                39000     \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 39,521\n",
      "Trainable params: 39,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 6s 4ms/step - loss: 6.0282 - acc: 0.5320\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 1s 1ms/step - loss: 0.8752 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 2s 1ms/step - loss: 0.6998 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 2s 1ms/step - loss: 0.6906 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 2s 1ms/step - loss: 0.6902 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 31\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 50\n",
      "Dense Neurons     = 10\n",
      "Dropout LSTM      = 0.25\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 30\n",
      "Kernel Init.      = glorot_uniform\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_32 (LSTM)               (None, 50)                39000     \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 39,521\n",
      "Trainable params: 39,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 5s 4ms/step - loss: 14.6266 - acc: 0.5028\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 0s 243us/step - loss: 5.8605 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 0s 266us/step - loss: 2.7824 - acc: 0.5405\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1406/1406 [==============================] - 0s 271us/step - loss: 1.6772 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 0s 264us/step - loss: 1.2129 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 32\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 50\n",
      "Dense Neurons     = 10\n",
      "Dropout LSTM      = 0.25\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 30\n",
      "Kernel Init.      = glorot_normal\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_33 (LSTM)               (None, 50)                39000     \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 39,521\n",
      "Trainable params: 39,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 6s 4ms/step - loss: 14.6335 - acc: 0.5398\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 0s 270us/step - loss: 6.2814 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 0s 246us/step - loss: 3.0729 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 0s 266us/step - loss: 1.7796 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 0s 246us/step - loss: 1.2214 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 33\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 100\n",
      "Dense Neurons     = 5\n",
      "Dropout LSTM      = 0.1\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 7\n",
      "Kernel Init.      = glorot_uniform\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_34 (LSTM)               (None, 100)               98000     \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 5)                 505       \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 98,511\n",
      "Trainable params: 98,511\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1406/1406 [==============================] - 7s 5ms/step - loss: 4.7547 - acc: 0.5519\n",
      "Epoch 2/5\n",
      "1406/1406 [==============================] - 2s 1ms/step - loss: 0.7239 - acc: 0.5405\n",
      "Epoch 3/5\n",
      "1406/1406 [==============================] - 2s 1ms/step - loss: 0.6912 - acc: 0.5405\n",
      "Epoch 4/5\n",
      "1406/1406 [==============================] - 1s 1ms/step - loss: 0.6893 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "1406/1406 [==============================] - 2s 1ms/step - loss: 0.6899 - acc: 0.5405\n",
      "\n",
      "Accuracy: 53.12%\n",
      "\n",
      "Confusion Matrix\n",
      "====================\n",
      "          pred:no  pred:yes\n",
      "true:no         0       165\n",
      "true:yes        0       187\n",
      "\n",
      "Clasification Report\n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       165\n",
      "           1       0.53      1.00      0.69       187\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       352\n",
      "   macro avg       0.27      0.50      0.35       352\n",
      "weighted avg       0.28      0.53      0.37       352\n",
      "\n",
      "=================================================================\n",
      "REPORT # 34\n",
      "=================================================================\n",
      "Training Parameters\n",
      "Number of lags    = 0\n",
      "Training Data %   = 75.0%\n",
      "=================================================================\n",
      "Model Parameters\n",
      "LSTM Neurons      = 100\n",
      "Dense Neurons     = 5\n",
      "Dropout LSTM      = 0.1\n",
      "Dropout Dense1    = 0.25\n",
      "Dropout Dense2    = 0.25\n",
      "Dropout Recurrent = 0.1\n",
      "L2 - LSTM Layer   = 0.1\n",
      "L2 - Dense Layer  = 0.25\n",
      "Batch Size        = 7\n",
      "Kernel Init.      = glorot_normal\n",
      "Epochs            = 5\n",
      "=================================================================\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_35 (LSTM)               (None, 100)               98000     \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 5)                 505       \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 98,511\n",
      "Trainable params: 98,511\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# run all simulations\n",
    "run_simulations(uniques=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
